\chapter{Introduction}
Earth observation has become an indispensable tool for understanding and monitoring the planet’s dynamic processes. Optical and radar remote sensing represent two complementary modalities at the core of modern Earth observation systems. Optical sensors provide rich spectral and visual information suitable for human interpretation. This information is used, among others, in forest and agricultural monitoring ~\cite{S2MS_GAN}. However, they are inherently constrained by atmospheric conditions such as illumination variability and in particular cloud cover. This causes considerable data gaps in both the spatial and temporal domains~\cite{CR_SEN2_dRNN}. 
In contrast, synthetic aperture radar (SAR) sensors operate in the microwave domain, offering all-weather, day-and-night imaging capabilities independent of sunlight or cloud interference. However, the backscatter-based nature of SAR imagery introduces challenges related to speckle noise, geometric distortions, and the absence of spectral color information~\cite{bench_sar_color, CR_RS_GAN_s2o}, necessitating advanced interpretation and analysis skills.

To bridge the gap between these two sensing modalities, SAR-to-optical image translation has emerged as a powerful generative approach. It aims to synthesize optical-like, cloud-free imagery from SAR data, combining the interpretability of optical observations with the advantages of radar acquisitions. Recent advances in generative artificial intelligence (GenAI), particularly in conditional generative adversarial networks (cGANs)~\cite{cGANs_mirza} and diffusion models~\cite{DDPM_2020}, have made it possible to learn complex mappings between SAR and optical domains with remarkable realism~\cite{cr_fuse_HR_GEN}. 
These developments have opened new pathways for applications in land-cover classification, vegetation monitoring, disaster response, and particularly, cloud removal. % keep or delete?

Despite the rapid progress in this field, most existing studies have focused primarily on reconstructing the visible RGB subset of optical imagery such as~\cite{CR_RS_GAN_s2o, cr_fuse_HR_GEN, c_diffusion_s2o, hvt_cgan, s2o_color_super_diff}, with a few extending to the NIR range as~\cite{filmy_cloudy_NIR_Enomoto, Mult_temp_S12_fus}, leaving the full multispectral potential of missions such as Sentinel-2 largely underexplored. Furthermore, while SAR-to-optical translation is often evaluated qualitatively, systematic assessments of its reliability across individual spectral bands remain limited. Finally, although the approach shows promise for cloud removal, its effectiveness in this context has not yet been comprehensively validated across the full spectral range

Filling the research gap, the present thesis investigates the use of generative models for translating Sentinel-1 SAR imagery into multispectral Sentinel-2 optical data. The work is guided by three main objectives:
\begin{enumerate}
    \item To validate SAR-to-optical image translation across the full 13 Sentinel-2 spectral bands. 
    \item To assess how reliably each optical band can be individually reconstructed and to what extent.
    \item To evaluate the performance of SAR-to-optical image translation for cloud removal
\end{enumerate}

Through these objectives, this thesis aims to provide a comprehensive and quantitative understanding of SAR-to-optical translation as a multimodal learning problem, highlighting its strengths, limitations, and potential for operational cloud-free optical data generation.

\bigskip
The remainder of this thesis is organized as follows: 
Chapter~\ref{chapter:background} establishes the theoretical foundation by outlining the principles of remote sensing, introducing the Sentinel missions, and reviewing key concepts in cloud removal and generative artificial intelligence, with a focus on SAR-to-optical image translation. Building on this foundation, the latter sections of the same chapter review the related literature, highlighting existing methods for SAR–optical data fusion and cloud removal, and identifying the research gaps that this work aims to address. 
Chapter~\ref{chapter:methodology} presents the methodological framework, describing dataset preparation, preprocessing steps, and the adopted Pix2Pix model, together with its training setup and evaluation procedure. The experimental results are reported in Chapter~\ref{chapter:results}, which details the reconstruction performance across the full spectral range and assesses the model’s capability for cloud removal. 
To further analyze the model’s behavior, Section~\ref{chapter:ablation} presents ablation studies investigating the influence of different loss configurations and the impact of excluding specific spectral bands. Chapter~\ref{chapter:discussion} discusses the overall findings in relation to the research objectives, summarizing the main challenges encountered, the limitations of the approach, and directions for future work. Finally, Chapter~\ref{chapter:conclusion} concludes the thesis with a summary of key contributions and closing remarks. The appendices provide supplementary material that supports and extends the analyses presented in the main chapters. They include intermediate model outputs observed during training, bandwise grayscale reconstructions, seasonal evaluation results, and value distributions of individual optical bands.
