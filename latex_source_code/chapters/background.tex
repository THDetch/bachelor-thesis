\chapter{Background \& Motivation}
\label{chapter:background}
\section{Remote Sensing}
Remote sensing (RS) is commonly defined as the acquisition of information about an object through sensors without direct physical contact. This information is obtained by detecting and measuring the modifications the object induces in its surrounding fields, which may include electromagnetic, acoustic, or potential fields \cite{book_Physics_Techniques_RS}.
RS is characterized by its strong interdisciplinary nature. It draws upon a wide spectrum of fields, requiring practitioners to develop a broad foundational understanding of both natural and applied sciences. Effective research in remote sensing often involves collaboration with specialists in electromagnetic theory, spectroscopy, applied physics, geology, atmospheric sciences, oceanography, electrical engineering, and optical engineering \cite{book_Physics_Techniques_RS}.
Remote observations require an interaction of energy between the target and the sensor. In the case of passive sensors, the detected energy originates from external or natural sources, such as solar radiation reflected by the Earth's surface or thermal radiation emitted by the object itself. A prominent example is the \textit{Landsat} program \footnote{https://landsat.gsfc.nasa.gov/}, which represents the longest continuously operating Earth observation mission. Over several decades, Landsat has generated a continuous global record, contributing significantly to environmental monitoring and Earth system science.

By contrast, active sensors generate their own energy pulses to illuminate the target and subsequently measure the portion of the signal that is reflected or backscattered. This capability allows them to operate independently of solar illumination and under a wide range of environmental conditions, including day or night and, in the case of microwave systems, through cloud cover and adverse weather \cite{RS_platforms_survey}. The most widely used active sensing technologies are Radar (Radio Detection and Ranging) and LiDAR (Light Detection and Ranging). Radar systems transmit and receive microwave radiation, whereas LiDAR employs laser pulses in the optical domain. For Earth observation, radar remote sensing typically operates in three wavelength ranges: X-band (2.4–4.5 cm), C-band (4.5–7.5 cm), and L-band (15–30 cm)~\cite{radar_FE_lexikon}. Among these, the most commonly employed monitoring system is Synthetic Aperture Radar (SAR), which is discussed in greater detail in Section~\ref{subsec-sentinel1}. Both radar and LiDAR record properties of the reflected signals to derive information about the observed surfaces and objects.

The term \textit{Remote Sensing} was introduced in the early 1960s to denote techniques for observing the Earth from a distance, with particular reference to aerial photography, which represented the predominant sensing technology at that time \cite{book_Satellite_RS}.
With the advent of satellites, global and synoptic observations of Earth and other planetary environments have become possible. Earth-orbiting sensors provide essential data on atmospheric dynamics, cloud distribution, vegetation cover, and its seasonal variability. Their long-term operation and repetitive coverage enable the monitoring of rapidly changing processes, such as polar ice dynamics and tropical deforestation. Beyond Earth, planetary missions (orbiters, flybys, landers, and rovers) have extended similar observations to all major planets in the solar system. To date, every planet has been visited at least once \cite{book_Physics_Techniques_RS}.

The origins of remote sensing date back to the invention of photography in 1839, which soon after was applied to topographic mapping. By the mid-19th century, aerial photographs were obtained from balloons, followed later by kites, pigeons, and eventually airplanes—the latter marking a decisive step with Wilbur Wright's first aerial photographs in 1909. Aerial photography became essential during World War I and advanced further in the 1930s-1940s with the introduction of color and infrared-sensitive films, widely used during World War II for reconnaissance and camouflage detection \cite{book_Physics_Techniques_RS,book_Satellite_RS}.
The postwar decades brought rapid technological progress with the development of radar and synthetic aperture radar (SAR), enabling high-resolution imaging independent of daylight or weather. Early rocket experiments in the late 1940s foreshadowed the space age, initiated by the launch of Sputnik in 1957. NASA's TIROS-1 satellite (1960) delivered the first global meteorological observations, while the launch of Landsat-1 in 1972 introduced systematic multispectral Earth observation, a program that continues today as the longest-running record of land surface change \cite{book_Physics_Techniques_RS,book_Satellite_RS}.

% A symbolic milestone came with the Apollo 8 mission in 1968, when astronaut William Anders captured the famous Earthrise photograph, showing Earth rising above the lunar horizon (see Figure \ref{fig:earthrise}). This image not only had profound cultural, philosophical, and scientific impact but also highlighted the scientific value of spaceborne Earth observation.
% \begin{figure}[!htbp]
%   \centering
%   \includegraphics[width=0.95\textwidth]{img/earthrise.jpg}
%   \caption[“Earthrise” photograph (Apollo 8, 1968)]{The iconic “Earthrise” photograph taken by astronaut William Anders during the Apollo 8 mission in 1968. Source: NASA.}
%   \label{fig:earthrise}
% \end{figure}
Since the 1980s, remote sensing has expanded through international efforts such as SPOT (France, 1986), MOS-1 (Japan, 1987), and IRS-1 (India, 1988). The European Space Agency (ESA)~\footnote{https://www.esa.int/} launched its first radar satellite, ERS-1, in 1991, and a second with comparable specifications in 1995. The 1990s and 2000s saw the rise of commercial satellites like IKONOS and QuickBird, offering very high-resolution imagery. Today, constellations of small satellites operated by private companies provide near-daily global coverage at meter-scale resolution. These advances—driven by improvements in optics, sensors, data transmission, and digital processing—have transformed remote sensing into a cornerstone of Earth system science, environmental monitoring, disaster response, and planetary exploration \cite{book_Satellite_RS}.

A summary of major milestones in the historical development of remote sensing platforms, from early balloon photography to modern satellite constellations, is illustrated in Figure~\ref{fig:RS_timeline}.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{img/RS_timeline.png}
  \caption[Timeline of remote sensing platform development]{Timeline of remote sensing platform development, from early airborne cameras to modern Earth observation satellites. Adapted from \cite{book_Satellite_RS}.}
  \label{fig:RS_timeline}
\end{figure}

\section{Copernicus: Europe's eyes on Earth}
Copernicus, the Earth observation component of the European Union's Space Programme, is widely regarded as the most ambitious environmental monitoring initiative to date. It is funded, coordinated, and managed by the European Commission in cooperation with partners such as the European Space Agency (ESA) and the European Organisation for the Exploitation of Meteorological Satellites (EUMETSAT)\footnote{https://www.eumetsat.int/}. Named after the astronomer Nicolaus Copernicus\footnote{https://www.biography.com/scientists/nicolaus-copernicus}, the programme integrates satellite and \textit{in situ} observations (e.g., ground, airborne, and seaborne instruments) to provide reliable, timely environmental information across six domains: land, marine, atmosphere, emergency management, security, and climate change.

The Copernicus Space Component consists of a dedicated series of satellites known as the \textsc{Sentinels}, numbered from Sentinel-1 to Sentinel-6, each designed to serve specific operational needs. Table~\ref{tab:sentinel_overview} summarises the main Sentinel missions, their sensing principles, observation domains, and primary applications. Together, they provide a comprehensive and complementary view of the Earth system.

\begin{table}[!htbp]
\centering
\caption[Overview of the Copernicus Sentinel missions]{Overview of the Copernicus Sentinel missions, summarising their sensors, observation domains, and main applications. Sources: \cite{ESA_Copernicus,ESA_SentinelMissions}.}
\label{tab:sentinel_overview}
\resizebox{\textwidth}{!}{
\begin{tabular}{lccp{7cm}}
\toprule
\textbf{Mission} & \textbf{Sensor Type / Band} & \textbf{Observation Domain} & \textbf{Main Applications / Purpose} \\
\midrule
\textsc{Sentinel-1} & C-band SAR & Land, Ocean, Ice & Monitoring land deformation, sea-ice dynamics, flooding, and earthquakes \\
\addlinespace
\textsc{Sentinel-2} & Multispectral Optical (13 bands) & Land, Vegetation, Water & High-resolution imaging for agriculture, forestry, land use, and disaster management \\
\addlinespace
\textsc{Sentinel-3} & Optical, Thermal, and Radar Instruments & Ocean, Land & Measuring sea surface topography, temperature, and colour for oceanography and climate studies \\
\addlinespace
\textsc{Sentinel-4} & UV–VIS–NIR Spectrometer & Atmosphere & Monitoring atmospheric composition and air quality over Europe and North Africa \\
\addlinespace
\textsc{Sentinel-5P} & UV–VIS–NIR Spectrometer & Atmosphere & Observing air quality, ozone, and UV radiation for climate and pollution assessment \\
\addlinespace
\textsc{Sentinel-6} & Radar Altimeter & Ocean & High-precision monitoring of sea level rise and ocean circulation \\
\bottomrule
\end{tabular}
}
\end{table}

Looking ahead, six Sentinel Expansion missions will extend the Copernicus capabilities, including hyperspectral imaging, polar topography, and carbon dioxide monitoring \cite{ESA_Copernicus}. As this work focuses on SAR and optical data, only Sentinel-1 and Sentinel-2 are discussed in detail in the following sections.


\subsection{Sentinel-1}
\label{subsec-sentinel1}
Sentinel-1, launched on 3 April 2014, constitutes the radar component of the European Copernicus Programme. The mission is designed as a constellation of two sun-synchronous, near-polar orbiting satellites in the same orbital plane, separated by 180° in phase. Equipped with C-band synthetic aperture radar (SAR) operating at 5.4~$\sim$~GHz ( corresponding to a wavelength of $\sim$5.55 cm), Sentinel-1 provides continuous, all-weather, day-and-night imaging capability. Since Sentinel-1 operates at microwave wavelengths, it is capable of measuring the physical properties of the target surface, whereas optical sensors are predominantly responsive to its chemical composition~\cite{radar_FE_lexikon}. Sentinel-1A was followed by Sentinel-1B in 2016, which ceased operations after an anomaly in 2021 and was subsequently replaced by Sentinel-1C in 2024. The SAR instrument actively transmits microwave signals towards the Earth and records the backscattered response. Both amplitude and phase are preserved, enabling the reconstruction of high-resolution images. Polarisation diversity further enhances information extraction, as different surfaces exhibit characteristic scattering signatures, supporting classification and retrieval of geophysical parameters.

Sentinel-1 operates in four exclusive acquisition modes: Stripmap (SM), Interferometric Wide Swath (IW), Extra-Wide Swath (EW), and Wave (WV). These modes achieve spatial resolutions down to $\sim$~5m and swath widths of up to  $\sim$~400km. The system supports single (HH or VV) and dual (HH+HV or VV+VH) polarisation, where H and V denote horizontal and vertical signal orientations, respectively. In this context, HH indicates that the radar transmits and receives signals horizontally, while VV corresponds to vertical transmission and reception. For HV and VH, the radar transmits in one orientation and receives in the orthogonal one, capturing cross-polarised backscatter information.
While SM, IW, and EW modes allow a duty cycle of up to 30 minutes per orbit, WV mode extends this to 75 minutes. Over land, IW mode with VV+VH polarisation is the primary operational configuration, balancing revisit performance, service requirements, and the creation of a consistent long-term archive. For open-ocean observations, WV mode with VV polarisation is predominantly employed, while EW mode is mainly used for sea-ice monitoring and maritime surveillance in high-latitude regions. SM mode is activated only for small islands or in response to emergencies. Across all modes, products are provided at multiple processing levels, from raw SAR data (Level-0) to geophysical ocean products (Level-2 OCN).

The revisit capabilities of Sentinel-1 are particularly notable. In IW mode, a single satellite can achieve global coverage every 12 days, while the two-satellite constellation reduces the repeat cycle to six days, completing 175 orbits per cycle. These systematic observations, combined with advanced interferometric capabilities, enable the precise detection of land subsidence, structural deformation, and ground movements that are otherwise imperceptible. Such data are invaluable for urban planning, geohazard monitoring, and applications in mining, geology, and risk assessment for infrastructure and natural hazards \cite{sentiwiki}.

\subsection{Sentinel-2}
Sentinel-2 is the optical imaging mission of the Copernicus Programme, designed to provide systematic, high-resolution observations over land and coastal regions. The mission consists of a constellation of two sun-synchronous satellites in the same orbital plane, phased 180° apart, ensuring global coverage with a revisit frequency of five days at the Equator. Sentinel-2A was launched in 2015, followed by Sentinel-2B in 2017 and Sentinel-2C in September 2024, the latter ensuring mission continuity as Sentinel-2A approaches the end of its operational lifetime.
Each satellite carries a single payload: the Multi-Spectral Instrument (MSI). This passive optical sensor collects sunlight reflected from the Earth’s surface, splitting the incoming radiation into two focal plane assemblies: one covering the visible and near-infrared (VNIR) and the other the shortwave infrared (SWIR). The instrument has a swath width of 290~km, which is considerably wider than comparable missions such as Landsat 5/7 (185~km) or SPOT-5 (120~km).

The MSI samples 13 spectral bands at three spatial resolutions: four bands at 10~m (Blue, Green, Red, and Near-Infrared), six bands at 20~m (red-edge and SWIR), and three bands at 60~m (aerosol, water vapour, and cirrus). These bands span the VNIR to SWIR regions of the electromagnetic spectrum and are tailored to applications including vegetation and crop monitoring, land cover mapping, water quality assessment, snow and ice monitoring, cloud screening, and atmospheric correction. An overview of the spectral bands is provided in Table~\ref{tab:sentinel2_bands}.
\begin{table}[!htbp]
\centering
\caption[Sentinel-2 MSI spectral bands]{Sentinel-2 MSI spectral bands with central wavelength and spatial resolution \cite{sentiwiki}.}
\label{tab:sentinel2_bands}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Band} & \textbf{Central Wavelength [nm]} & \textbf{Resolution [m]} \\ \midrule
B1  & 443  (Aerosols)                & 60 \\
B2  & 490  (Blue)                    & 10 \\
B3  & 560  (Green)                   & 10 \\
B4  & 665  (Red)                     & 10 \\
B5  & 705  (Red edge)                & 20 \\
B6  & 740  (Red edge)                & 20 \\
B7  & 783  (Red edge)                & 20 \\
B8  & 842  (NIR)                     & 10 \\
B8a & 865  (Red edge)                & 20 \\
B9  & 945  (Water vapour)            & 60 \\
B10 & 1375 (Cirrus)                  & 60 \\
B11 & 1610 (SWIR)                    & 20 \\
B12 & 2190 (SWIR)                    & 20 \\ \bottomrule
\end{tabular}
\end{table}

Sentinel-2 imagery is systematically and freely available, supporting several Copernicus services, including the Copernicus Land Monitoring Service (CLMS), the Copernicus Marine Environment Monitoring Service (CMEMS), and the Copernicus Emergency Management Service (CEMS). These services, along with selected applications and their respective launch years, are illustrated in Table~\ref{tab:sen2_services}. Through its systematic, frequent, and global observations, Sentinel-2 has become a cornerstone of the Copernicus programme, enabling comprehensive environmental monitoring, sustainable resource management, and rapid disaster response worldwide~\cite{sentiwiki}.

\begin{table}[!htbp]
  \centering
  \caption[Sentinel-2 Copernicus Services applications]{Overview of Sentinel-2 Copernicus Services and their applications.}
  \label{tab:sen2_services}
  \begin{adjustbox}{max width=\textwidth, keepaspectratio=false}
    \begin{tabular}{p{7.5cm} p{3.5cm} p{9.5cm}}
      \toprule
      \textbf{Copernicus Service} & \textbf{Operational Since} & \textbf{Sentinel-2 Application(s)} \\
      \midrule
      Copernicus Land Monitoring Service (CLMS) & 2012 &
      \begin{itemize}[nosep,leftmargin=*]
        \item Land cover and forest mapping
        \item Crop monitoring
        \item Ecosystem assessment
        \item Climate change adaptation
      \end{itemize} \\
      \midrule
      Copernicus Emergency Management Service (CEMS) & 2012 &
      \begin{itemize}[nosep,leftmargin=*]
        \item Disaster response and rapid mapping
        \item Flood monitoring
        \item Fire assessment
        \item Earthquake damage mapping
      \end{itemize} \\
      \midrule
      Copernicus Marine Environment Monitoring Service (CMEMS) & 2014 &
      \begin{itemize}[nosep,leftmargin=*]
        \item Turbidity estimation
        \item Chlorophyll concentration mapping
        \item Suspended particulate matter analysis
        \item Bathymetry derivation
        \item Ice monitoring and analysis
      \end{itemize} \\
      \bottomrule
    \end{tabular}
  \end{adjustbox}
\end{table}

Together, Sentinel-1 and Sentinel-2 provide complementary SAR and optical observations, which form the basis of this thesis aiming to translate SAR imagery into its optical counterpart.

\section{Cloud Removal}
As briefly mentioned in the sections above, optical remote sensing imagery, such as Sentinel-2 products, represents a key source of Earth observation data. Compared to SAR observations, multispectral images contain rich spectral information and are readily interpretable by the human eye. Such data play an essential role in a wide range of applications, including environmental monitoring, resource exploration, and disaster assessment. While the quality and quantity of satellite observations have dramatically increased in recent years, one common problem persists for optical remote sensing imagery: \textbf{cloud cover}.

Based on findings from the International Satellite Cloud Climatology Project (ISCCP), average global cloud cover surpasses 66\% \cite{dl_cloud_detection_survey,A_cGAN_fuse_sar_MS_CR,CR_SEN2_dRNN}, with 55\% over land surface alone \cite{CR_SEN2_dRNN}, preventing optical satellites from acquiring valuable information about the Earth's surface due to the frequent presence of clouds in the imagery. In contrast to SAR instruments, optical sensors cannot penetrate clouds, resulting in considerable data gaps in both the spatial and temporal domains. For applications requiring consistent time series, e.g., agricultural monitoring, or where a specific scene must be observed at a given time, e.g., disaster monitoring, cloud cover represents a serious limitation \cite{CR_SEN2_dRNN}. The diversity of clouds —including thin and thick clouds as well as haze— together with the wide range of occlusion scenarios and their uneven distribution, poses an additional challenge for image reconstruction and the generalizability of cloud removal techniques\cite{CR_Advances_Review_ORS}.

Consequently, removing clouds and obtaining cloud-free optical data to retrieve surface information is both of theoretical importance and practical necessity. Cloud removal in optical remote sensing imagery aims to mitigate or eliminate the influence of clouds, thereby revealing more accurate and complete surface details \cite{CR_Advances_Review_ORS}. In response to this challenge, a wide range of approaches have been proposed. These methods can broadly be divided into three categories: (i) single-image methods, (ii) multimodal-based methods, and (iii) multitemporal-based methods \cite{CR_Advances_Review_ORS}. The main categories and their characteristics are summarized below:

\begin{enumerate}[label=(\Alph*)]
  \item \textbf{Single-image methods:} Constrained by the limited acquisition capabilities of early remote sensing data, single-image cloud removal techniques attempt to restore surface information using only the cloudy optical image. Classical approaches employ statistical and physical models such as spatial similarity, frequency filtering, or atmospheric scattering models. For example, Zhang et al. \cite{single_variation} proposed the \emph{Haze Optimized Transformation (HOT)}, which detects and compensates for thin cloud and haze contamination in Landsat images by exploiting the spectral correlation of clear-sky bands and quantifying deviations caused by haze. Similarly, He et al. \cite{single_haze_removal_dark_prior} introduced the \emph{dark channel prior}, a widely used statistical prior that estimates haze thickness from local image patches to recover clear radiance, later adapted for thin cloud removal in optical remote sensing. With the advent of deep learning, CNNs, U-Nets, and GAN-based architectures have been applied to learn the mapping from cloudy to cloud-free domains, sometimes extended with unpaired learning schemes like CycleGANs. While these methods demonstrate effectiveness for thin or semi-transparent clouds, their reliance on information present in a single image limits their applicability to dense cloud cover. In such cases, they cannot reliably reconstruct surface features, which has motivated the integration of external data sources such as SAR imagery \cite{CR_Advances_Review_ORS}.

  \item \textbf{Multimodal-based methods:} Multimodal strategies explicitly integrate auxiliary data from complementary sensors to enhance optical image restoration. While multispectral approaches exploit the differential sensitivity of spectral bands, the most significant progress has been achieved through the fusion of synthetic aperture radar (SAR) and optical imagery.A representative work has been introduced by Meraner et al. \cite{CR_SEN2_dRNN}, who proposed the DSen2-CR framework, a deep residual network that combines Sentinel-1 and Sentinel-2 data to improve reconstructions under thick cloud cover and preserve spectral fidelity. Likewise, Grohnfeldt et al.~\cite{A_cGAN_fuse_sar_MS_CR} demonstrated the potential of conditional GANs (cGANs) to fuse SAR and multispectral data for cloud removal, highlighting the advantages of adversarial training in capturing nonlinear relationships between modalities. More recently, Xu et al. \cite{GLF_CR} presented the GLF-CR model, which applies a global–local fusion strategy to better exploit SAR features for cloud removal. SAR-to-optical image translation has thus emerged as a powerful paradigm in this context, as SAR penetrates cloud layers and provides structural information that can guide optical reconstruction. A wide range of approaches have been proposed, including CNN-based fusion, cGANs, and CycleGAN-style frameworks, which either translate SAR features into optical-like imagery or combine them with partially corrupted optical inputs. These methods have proven especially effective in recovering surface information under dense and persistent cloud conditions, although challenges remain in terms of data registration, modality differences, and SAR-induced speckle noise.

  \item \textbf{Multitemporal-based methods:} Multitemporal approaches leverage repeated acquisitions of the same location at different times to fill in cloud-covered areas. \textit{Non-blind} methods use cloud masks to guide restoration, whereas \textit{blind} methods directly infer cloud-free information from temporal sequences. A classical example is the work of Xu et al. \cite{CR_spars_repre_MT_dict_L}, who proposed a sparse representation framework with multitemporal dictionary learning (MDL) that learns dictionaries from both cloudy and clear images, effectively reconstructing areas obscured by thin and thick clouds without requiring explicit cloud masks. More recently, Ebel et al. \cite{UnCRtainTS} introduced UnCRtainTS, an attention-based deep learning model that not only reconstructs cloud-free images from Sentinel-1/2 time series but also quantifies pixel-wise uncertainty, providing reliability measures alongside the reconstructed outputs. Techniques therefore range from traditional model-driven approaches, such as low-rank tensor decomposition and sparse representation, to data-driven deep learning frameworks that learn spatio-temporal mappings. Recent research has also begun to combine multitemporal optical data with SAR, creating hybrid SAR–optical time series methods that enhance robustness under persistent cloud cover and enable more accurate SAR-to-optical translation. Although highly effective for dense cloud removal, these approaches face challenges such as geometric misalignment, temporal variability in land cover \cite{expl_ML_CR_Cameroon}, and the need for large, paired training datasets \cite{CR_Advances_Review_ORS}. In this context, mono-temporal data offers an advantage, as it requires less data and avoids the need for co-registration compared to multi-temporal approaches~\cite{kiwa_auto_Delineation_BAs}.
\end{enumerate}

\begin{table}[ht]
  \centering
  \caption[Cloud removal method categories]{Summary of cloud removal categories, their advantages and limitations \cite{CR_Advances_Review_ORS, sar_2_opt_CGAN_survey_taxonomy}.}
  \label{tab:cloud_removal_categories}
  \begin{adjustbox}{max width=\textwidth, keepaspectratio=false}
    \begin{tabular}{p{2.5cm} p{6cm} p{6cm} p{3cm}}
      \toprule
      \textbf{Category}                                                                       & \textbf{Advantages} & \textbf{Limitations} & \textbf{Representative literature}                                                                                                                         \\
      \midrule
      \textbf{Single-image}                                                                   &
      \begin{itemize}[nosep,leftmargin=*]
        \item No auxiliary data required (cost- and time-efficient).
        \item Effective for thin or semi-transparent clouds.
        \item Straightforward implementation with statistical/physical models or deep learning.
      \end{itemize} &
      \begin{itemize}[nosep,leftmargin=*]
        \item Ineffective for dense or opaque clouds.
        \item Often introduces artifacts or color distortions.
        \item Deep learning requires large paired datasets, which are difficult to obtain.
      \end{itemize}      &
      \cite{single_variation} \cite{single_haze_removal_dark_prior} \cite{single_artifact_free_CR_GAN} \cite{single_thin_CR_ORS_GAN_phys} \cite{single_multi_DR_CR} \cite{single_CR_DLM_matting} \cite{single_AGLC_GAN} \cite{single_PNBT_CR} \cite{single_CGAN_scattering_martian}                     \\
      \midrule
      \textbf{Multimodal}                                                                     &
      \begin{itemize}[nosep,leftmargin=*]
        \item Integrates complementary information from other sensors.
        \item Multispectral bands provide spectral redundancy.
        \item SAR–optical fusion enables SAR-to-optical translation, penetrating cloud layers.
        \item Suitable for both thin and thick clouds.
      \end{itemize}  &
      \begin{itemize}[nosep,leftmargin=*]
        \item Requires accurate registration of heterogeneous data.
        \item SAR data introduces speckle noise.
        \item High computational complexity and preprocessing effort.
      \end{itemize}                           &
      \cite{A_cGAN_fuse_sar_MS_CR} \cite{sar2opt_cGAN_Optim_oppr_limits} \cite{syn_ms_sar_opt_MT_cGAN} \cite{CR_SEN2_dRNN} \cite{GAN_gen_synt_MS} \cite{s2o_ViT_cGAN} \cite{CR_RS_GAN_s2o} \cite{s2o_Thermodynamics} \cite{c_diffusion_s2o} \cite{s2o_color_super_diff} \cite{S2MS_GAN} \cite{SAR_DeCR} \\
      \midrule
      \textbf{Multitemporal}                                                                  &
      \begin{itemize}[nosep,leftmargin=*]
        \item Exploits temporal redundancy to reconstruct cloudy regions.
        \item Effective for dense and extensive cloud cover.
        \item Deep learning models can capture spatio-temporal correlations.
        \item Can be extended with SAR–optical time series for improved robustness.
      \end{itemize}             &
      \begin{itemize}[nosep,leftmargin=*]
        \item Sensitive to geometric misalignment and temporal variability.
        \item Requires consistent multitemporal datasets, which may be unavailable.
        \item Landscape or seasonal changes reduce restoration accuracy.
      \end{itemize}             &
      \cite{syn_ms_sar_opt_MT_cGAN} \cite{CR_RS_spati_atten_GAN} \cite{CR_spars_repre_MT_dict_L} \cite{UnCRtainTS} \cite{assessing_MT_cGANS_s2o_crop} \cite{DiffCR}                                                                                                                                     \\
      \bottomrule
    \end{tabular}
  \end{adjustbox}
\end{table}

As shown in Table~\ref{tab:cloud_removal_categories}, research on cloud removal has been uneven across categories. Single-image methods have been the most extensively studied due to their simplicity and minimal data requirements; however, their effectiveness is limited under dense cloud conditions. Multimodal approaches, particularly SAR–optical fusion, have gained significant traction in recent years and currently represent the most active research direction. By contrast, multitemporal methods, while highly effective in principle, have been less frequently explored because of the challenges associated with acquiring consistent and well-aligned time-series data. Another challenging aspect is the change in ground surface conditions between multitemporal acquisitions, which can confuse the reconstruction process.

In summary, cloud removal research spans single-image, multimodal, and multitemporal strategies, each with distinct advantages and limitations. Among these, SAR-to-optical image translation has recently emerged as a particularly promising direction, as it leverages the cloud-penetrating capability of SAR while producing optical-like imagery suitable for interpretation and analysis. This thesis builds on this line of research by systematically investigating and advancing SAR-to-optical translation methods for cloud removal.

\section{Generative Artificial Intelligence}
\label{sec:genAI}
Generative Artificial Intelligence (GenAI) refers to a class of machine learning models designed to generate new data samples that resemble a given training distribution, such as images, text, or audio. Unlike discriminative models, which focus on classifying or predicting labels, generative models learn (an approximation to) the underlying probability distribution of the data to create novel instances~\cite{genAI_HS_sens_data_review}. This capability has revolutionized fields like computer vision, where GenAI is used for tasks including image synthesis, style transfer, and domain adaptation. In the context of remote sensing, GenAI enables, among others, the creation of synthetic imagery, such as translating radar data to optical-like representations, which is particularly useful for overcoming environmental limitations like cloud cover, as well as data fusion for both heterogeneous and homogeneous imagery, enhancing spatial, spectral, and temporal resolution and mitigating the limitations of individual sensors~\cite{RS_Data_Fusion_GANs_sota}.

One of the foundational frameworks in GenAI is the Generative Adversarial Network (GAN), introduced in 2014 by Goodfellow et~al.~\cite{GANs_Goodfellow}. As depicted in Figure~\ref{fig:GANs}, a GAN consists of two neural networks: a generator ($G$) that produces synthetic data from random noise, and a discriminator ($D$) that evaluates whether the generated data is real or fake.
These components are trained adversarially—the generator aims to fool the discriminator, while the discriminator improves its ability to distinguish real from generated samples—leading to increasingly realistic outputs.
\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.95\textwidth]{img/GANs.png}
  \caption[GAN architecture overview]{GAN Architecture Overview: Source: https://developers.google.com/}
  \label{fig:GANs}
\end{figure}

This adversarial process minimizes a minimax loss function, allowing GANs to capture complex data distributions without explicit probabilistic modeling. Formally, the optimization problem is defined as:
\begin{equation}
  \min_{G} \max_{D} \; V(D,G) = \mathbb{E}_{x \sim p_{\text{data}}(x)}[\log D(x)]
  + \mathbb{E}_{z \sim p_{z}(z)}[\log (1 - D(G(z)))] ,
\end{equation}

where $D(x)$ denotes the discriminator’s estimate of the probability that $x$ comes from the real data distribution $p_{\text{data}}$, and $G(z)$ generates samples from latent noise $z \sim p_z$.

Building on GANs, conditional GANs (cGANs)~\cite{cGANs_mirza} incorporate additional input conditions, such as class labels or reference images, to guide the generation process toward specific outputs. In image-to-image translation settings, cGANs typically require paired datasets for supervised training, which is non-trivial given the complexity of acquiring and registering satellite imagery. A key extension for unpaired image translation is the Cycle-Consistent GAN (CycleGAN)~\cite{cycle_GANs}, proposed in 2017. CycleGAN addresses the challenge of learning mappings between two domains (e.g., SAR to optical) without paired training examples by enforcing cycle consistency: translating an image from domain $X$ to $Y$ and back to $X$ should reconstruct the original. This is achieved through a cycle-consistency loss, combined with adversarial losses, making it suitable for RS applications where perfectly aligned SAR–optical pairs are scarce. Nevertheless, given the weak supervision during CycleGAN training, it is prone to texture/detail distortions~\cite{RS_Data_Fusion_GANs_sota}, often resulting in performance comparable to other GAN-based models~\cite{assessing_MT_cGANS_s2o_crop}.
For the problem of RS data fusion, many studies do not directly reuse off-the-shelf GAN architectures but instead adapt them to better accommodate multisource fusion. Among these, cGANs and CycleGANs are the most commonly adopted families in numerous fusion studies~\cite{RS_Data_Fusion_GANs_sota}.

Despite the remarkable performance of GAN-based approaches in cloud removal, several challenges remain. They are inherently difficult to train and can suffer from issues such as mode collapse, leading to distorted or repetitive outputs, especially at high spatial resolutions~\cite{GAN_gen_synt_MS}. Moreover, cloud morphology and distribution exhibit substantial diversity and complexity, imposing demanding requirements on training and application of GAN-based methods~\cite{DiffCR, CR_Advances_Review_ORS}.

More recently, diffusion models have emerged as a powerful alternative to GANs, offering improved stability and higher-fidelity generation~\cite{c_diffusion_s2o}. Denoising Diffusion Probabilistic Models (DDPMs)~\cite{DDPM_2020}, introduced in 2020, model data generation as a reverse diffusion process: starting from Gaussian noise, the model iteratively denoises the input to produce a sample from the target distribution. Unlike GANs, which can suffer from mode collapse (limited output diversity), diffusion models provide probabilistic sampling and better coverage of diverse data modes. In RS, diffusion-based approaches are gaining traction for tasks like cloud removal, where they can generate realistic optical images by conditioning on SAR inputs.

GenAI's application in remote sensing, particularly for multimodal data fusion, leverages these models' ability to bridge domain gaps. For instance, GANs and diffusion models can synthesize cloud-free optical imagery from SAR, preserving structural details while enhancing interpretability. However, challenges such as training instability in GANs and high computational costs in diffusion models persist, motivating ongoing research into hybrid architectures. Further details regarding the applied approaches are provided in the following section.

\section{SAR-to-optical image translation}
Optical imagery provides rich spectral information and can often be interpreted without expert knowledge, but it is highly sensitive to atmospheric conditions such as cloud cover, which frequently limits its usability. In contrast, SAR imagery offers all-weather, day-and-night, cloud-penetrating capabilities, though its complex backscatter characteristics, speckle noise, and lack of color make interpretation challenging even for experts~\cite{s2o_color_super_diff}. Moreover, while two objects with identical structures may appear different in optical imagery due to their spectral responses, they can appear similar in SAR imagery, reflecting SAR’s emphasis on structural rather than spectral properties~\cite{RS_Data_Fusion_GANs_sota}. Bridging this gap, SAR-to-optical translation generates synthetic, cloud-free optical-like images from SAR data, combining the interpretability of optical imagery with SAR’s robustness to clouds. Defined as an image-to-image translation (I2I) task, this process is valuable for applications requiring consistent cloud-free optical information, such as land-cover classification, disaster monitoring, and vegetation analysis~\cite{CR_SEN2_dRNN}.

The domain gap between SAR and optical imagery, however, poses significant challenges. SAR images exhibit speckle noise due to coherent interference, geometric distortions from side-looking geometry, and intensity-based representations that differ fundamentally from the reflectance-based multispectral bands of optical sensors~\cite{sar2opt_cGAN_Optim_oppr_limits}. Acquiring perfectly co-registered SAR–optical pairs is also non-trivial, as both spatial and temporal alignment must be ensured. According to Tobler’s First Law of Geography—\textit{"everything is related to everything else, but near things are more related than distant things"}~\cite[p. 236]{Tobler_1st_law}—the closer the distance and the shorter the time interval, the greater the correlation between features~\cite{sar_2_opt_CGAN_survey_taxonomy}. Achieving such conditions in practice is difficult, as the two imaging modalities differ fundamentally; certain land features (e.g., roads, playgrounds, airport runways) may appear differently in terms of spectral reflectance versus SAR backscatter, complicating accurate cross-domain mapping~\cite{sar_2_opt_CGAN_survey_taxonomy}.  

Before the emergence of generative AI, SAR-to-optical translation relied on heuristic or classical methods. Early approaches used pseudo-colorization of SAR channels or polarization composites to enhance interpretability~\cite{colorization_Schmitt_2018}, though they failed to reproduce true optical characteristics. Multisensor fusion methods, such as combining SAR with prior cloud-free optical data via intensity–hue–saturation (IHS) or wavelet-based transforms~\cite{IHS_wavelet_Zhang2019}, provided partial solutions but depended on handcrafted features and complex preprocessing, limiting scalability and accuracy.  

Recent advances in deep learning have transformed this process. Generative Adversarial Networks (GANs)~\cite{GANs_Goodfellow} and diffusion models~\cite{DDPM_2020} enable end-to-end learning of mappings between SAR and optical domains, capturing pixel-level distributions and feature correlations for realistic, cloud-free optical reconstruction. Most prior work focuses on reconstructing visible RGB bands of Sentinel-2 imagery, with fewer studies extending into the near-infrared (NIR) or full multispectral range. In the literature, the former is often referred to as SAR-to-optical translation, while the latter is known as SAR-to-multispectral (SAR-to-MS) translation.

For the purposes of this thesis, the term \emph{optical} is used broadly to encompass the full spectral range of Sentinel-2. Accordingly, SAR-to-optical denotes translation tasks regardless of the number of reconstructed bands. Within this context, multispectral GAN-generated images are expected to exhibit both structural and spectral fidelity, ensuring suitability for quantitative remote sensing and cloud removal applications. Thus, SAR-to-optical translation can be regarded as a data-driven cloud removal strategy that bridges the interpretability of optical imagery with the robustness of SAR. This thesis extends the task to the complete multispectral domain of Sentinel-2 data.

The foundational GAN framework by Goodfellow et al.~\cite{GANs_Goodfellow}, later extended to Conditional GANs (cGANs) by Mirza and Osindero~\cite{cGANs_mirza}, enabled image-to-image translation tasks that made SAR-to-optical synthesis feasible. Fuentes Reyes et al.~\cite{sar2opt_cGAN_Optim_oppr_limits} optimized an unsupervised CycleGAN for SAR-to-optical translation, improving interpretability and reducing speckle through customized preprocessing and architectural refinements, although fine structural details remained difficult to preserve in urban scenes. Wang et al.~\cite{s2ot_s_cycle_adv} introduced a Supervised CycleGAN (S-CycleGAN) by incorporating a pixel-wise MSE loss, producing realistic optical images and showing strong potential for cloud removal. Gao et al.~\cite{cr_fuse_HR_GEN} extended this idea with a fusion-based GAN framework for high-resolution imagery. Instead of directly translating SAR to optical, they generated a simulated optical image from SAR data and fused it with both SAR and optical inputs to reconstruct more realistic results. An ablation study confirmed that this two-stage fusion strategy achieved superior performance among tested configurations.

Following the success of Vision Transformers (ViTs)~\cite{ViT_2020}, transformer-based architectures have also emerged. Zhao et al.~\cite{hvt_cgan} proposed the Hybrid Vision Transformer cGAN (HVT-cGAN), combining CNN and ViT branches to capture both local details and global semantics. A Convolutional Attention Fusion Module (CAFM) adaptively merged multiscale features, enhancing texture and color fidelity. Trained on the SEN1-2 dataset~\cite{sen12_2018}, HVT-cGAN achieved superior visual and quantitative results over previous GAN-based models. Park et al.~\cite{s2o_ViT_cGAN} further improved this approach with a multiscale ViT-based cGAN architecture integrating perceptual loss and a two-phase transfer learning strategy to enhance realism and stability.

Diffusion models have recently entered this field. Bai et al.~\cite{c_diffusion_s2o} introduced a conditional diffusion model for SAR-to-optical translation, which, despite limited experimentation due to computational constraints, demonstrated promising performance compared to GAN-based methods. Bai et al.~\cite{s2o_color_super_diff} later extended the framework with color supervision to improve reconstruction fidelity. A more recent contribution, the Multi-Temporal Conditional GAN (MTcGAN) by Kwak and Park~\cite{assessing_MT_cGANS_s2o_crop}, was designed for early-stage crop monitoring and utilized SAR–optical pairs from reference and prediction dates to capture temporal dynamics, achieving superior spectral consistency compared to conventional methods.

These developments demonstrate the rapid progression of SAR-to-optical translation, from GAN-based frameworks to transformer and diffusion models. Together, they form the foundation for applying SAR–optical fusion and translation techniques to cloud-contaminated optical imagery, as discussed in the following section.

\section{SAR-to-Optical Image Translation for Cloud Removal}
Cloud contamination in optical remote sensing imagery hinders continuous Earth observation, limiting applications such as crop monitoring and land-cover classification. Synthetic Aperture Radar (SAR) systems can penetrate clouds, enabling data acquisition in all weather conditions. This capability makes SAR data valuable for reconstructing missing optical information in cloud-affected areas and has driven increasing interest in SAR–optical fusion and translation for cloud removal. Early methods relied on traditional signal processing techniques. Huang et al.~\cite{huang2015} introduced sparse representation-based cloud removal using SAR data, which Xu et al.~\cite{CR_spars_repre_MT_dict_L} extended via multi-temporal dictionary learning. These approaches demonstrated the feasibility of SAR-assisted cloud reconstruction but struggled under heavy cloud cover or rapidly changing surface conditions.

With the emergence of deep learning, researchers began to exploit neural architectures for this task. Enomoto et al.~\cite{filmy_cloudy_NIR_Enomoto} applied a cGAN for cloud removal by fusing the RGB composite of a cloudy optical image with the cloud-free near-infrared (NIR) band to reconstruct a cloud-free RGB image. Although limited under dense cloud conditions, the approach was pivotal in demonstrating the potential of multimodal data fusion for cloud removal and laid the groundwork for subsequent studies. Building on this concept, Grohnfeldt et al.~\cite{A_cGAN_fuse_sar_MS_CR} introduced SAR-Opt-cGAN, a model designed to fuse Sentinel-1 SAR and Sentinel-2 optical data—marking the first use of SAR data within a cGAN framework for cloud mitigation. Their adaptation of the Pix2Pix architecture allowed flexible multi-channel inputs and was trained on a subset of the SEN1-2 dataset~\cite{sen12_2018}. The results confirmed the advantage of incorporating SAR data, validating the effectiveness of the approach for mitigating cloud cover.

The research group from the Technical University of Munich (TUM)~\footnote{https://www.tum.de/} and the German Aerospace Center (DLR)~\footnote{https://www.dlr.de/en} further advanced this field by developing a family of co-registered SAR–optical datasets. The initial SEN1-2 dataset~\cite{sen12_2018} combined single-polarized (VV) SAR with RGB optical composites, later extended to dual-polarized SAR and full 13-band optical imagery in SEN12-MS~\cite{sen12ms_2019}. To address cloud contamination directly, they released SEN12-MS-CR~\cite{sen12ms-cr_2021}, containing triplets of dual-polarized SAR, cloudy optical, and corresponding cloud-free images. The SEN12-MS-CR-TS dataset~\cite{sen12ms-cr-ts_2022} then introduced multi-temporal observations, greatly enhancing its relevance for time-series cloud-removal studies.

Naderi Darbaghshahi et al.~\cite{CR_RS_GAN_s2o} proposed a dual-GAN framework employing Dilated Residual Inception Blocks (DRIBs). The first GAN translated SAR data into optical imagery, while the second fused this output with a cloudy optical image to produce a cloud-free result. This design effectively removed cloudy regions while preserving clear areas, demonstrating strong qualitative performance despite moderate quantitative accuracy. Ebel et al.~\cite{UnCRtainTS} extended this direction by incorporating uncertainty prediction into multispectral reconstruction with their model UnCRtainTS, which outperformed prior methods and provided per-pixel uncertainty maps to indicate the reliability of spectral predictions.

Diffusion models have recently emerged as a competitive alternative for cloud removal. Zou et al.~\cite{DiffCR} presented DiffCR, a fast conditional diffusion framework for cloud removal from optical satellite imagery, which currently represents the state-of-the-art on the SEN12-MS-CR~\cite{sen12ms-cr_2021} dataset. Unlike traditional GAN-based or computationally intensive diffusion approaches, DiffCR employs a decoupled conditional architecture and a novel Time and Condition Fusion Block (TCFBlock) to efficiently fuse multiscale spatiotemporal features. By directly predicting clean cloud-free images instead of noise, it achieves faster convergence and remarkable fidelity, producing high-quality reconstructions in as few as 1–5 denoising steps with over 95\% lower computational cost than previous diffusion-based methods. Meraner et al.~\cite{CR_SEN2_dRNN} proposed DSen2-CR, a deep residual neural network for cloud removal in Sentinel-2 imagery that integrates SAR–optical data fusion to enhance reconstruction under thick cloud cover. Built upon the DSen2 super-resolution ResNet, the model introduces a Cloud-Adaptive Regularized Loss (CARL) to preserve uncorrupted input information while reconstructing only clouded regions. Trained and evaluated on the SEN12-MS-CR dataset, DSen2-CR demonstrated strong generalization across global scenes and outperformed GAN-based baselines in both spectral and structural fidelity. Across-band evaluation confirmed high reconstruction accuracy over all 13 Sentinel-2 bands, with the best performance on surface bands and slightly higher errors in atmospheric ones.

Recent advances such as DSen2-CR and DiffCR illustrate the maturity of SAR-to-optical cloud-removal research, highlighting a clear trend toward physically interpretable, data-rich, and computationally efficient architectures. Building upon this foundation, the present thesis validates SAR-to-optical image translation across the full 13 Sentinel-2 spectral bands and further investigates its applicability to practical cloud-removal scenarios.

\section{Application and Relevance to KIWA Project}
The KIWA project~\footnote{https://www.kiwa-projekt.de} \textit{(German: KI-basierte Waldüberwachung – Engl: AI-based Forest Monitoring)} addresses the growing ecological challenge of forest degradation and wildfire risk in Central Europe. Climate extremes, prolonged droughts, and pest infestations are severely affecting forests, making them increasingly vulnerable to fire. Wildfires not only destroy ecosystems, properties, and human lives, but also release the $\mathrm{CO_2}$ previously absorbed by forests, thereby accelerating climate change. Current monitoring methods, such as aircraft patrols and stationary watchtowers, are resource-intensive, costly, and limited in performance~\cite{THD_ZAF}.

To overcome these challenges, KIWA integrates artificial intelligence with advanced remote sensing technologies, particularly drones equipped with computer-vision systems, to improve early wildfire detection. The project’s broader objectives include delivering high-resolution environmental data, providing decision support to emergency services, and supporting climate-resilient, biodiversity-rich forest management. KIWA thus exemplifies an AI “lighthouse” initiative with the ambition to serve as a transferable blueprint for forest monitoring systems across Germany and internationally~\cite{KIWA_Project}.

Similar to the general challenges faced in remote sensing applications, the KIWA project requires gapless observation capabilities. For instance, recent KIWA-related research highlights the need for automated methods to delineate burned areas (BAs) and assess wildfire risks using remote sensing data~\cite{kiwa_auto_Delineation_BAs}. These approaches primarily rely on optical indices such as NDVI, NBR, NDWI, and WFI, which require cloud-free multispectral imagery. However, cloud cover and wildfire smoke often make it difficult to obtain a continuous historic record of the areas of interest, which is critical in emergency services. As the authors state, \textit{"excluding multi-temporal approaches per se for our KIWA workflow is not an option and not intended"}~\cite[p.~767]{kiwa_auto_Delineation_BAs}.

In this context, the contribution of this thesis—translating SAR data into optical-like images using generative AI models—offers a direct benefit to KIWA. Once validated, the proposed methods can be integrated in the project workflow to enhance the spatial and temporal coverage of forest monitoring, even under cloudy or adverse weather conditions. By extending the availability of optical-equivalent data in near real-time, these approaches can improve the robustness of KIWA’s mapping workflows, improve mono- and multi-temporal analyses, and provide more reliable support for decision-making processes in emergency services. This integration has the potential to enhance KIWA’s operational efficiency and transferability, supporting its mission to deliver automated, scalable, and accurate wildfire monitoring solutions.

